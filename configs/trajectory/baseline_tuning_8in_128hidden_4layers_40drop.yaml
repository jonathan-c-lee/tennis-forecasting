name: baseline_tuning_8in_128hidden_4layers_40drop
frames_in: 8
frames_out: 15
layers: 4
hidden_size: 128
dropout: 0.4
loss: MSE loss
optimizer: Adam
learning rate: 0.002
weight decay: 0.0001
epochs: 100
batch_size: 32
train loss:
- 0.03728246757218355
- 0.023228002558874934
- 0.02146830802335392
- 0.020350049721383597
- 0.018499841394869588
- 0.01649003055040972
- 0.014166386904124218
- 0.013726922235439849
- 0.0135559595036733
- 0.01332844567449787
- 0.01321189796339862
- 0.013044156181284143
- 0.013012758179131565
- 0.01251622197465806
- 0.012519573179792754
- 0.012499505724710754
- 0.012674959405926587
- 0.01246521368481313
- 0.012348544559901274
- 0.012069715310736924
- 0.012511702578609125
- 0.012299403076684928
- 0.012148293778536063
- 0.012198537906419627
- 0.012228274193321224
- 0.01214774621391221
- 0.011981729282467048
- 0.011866077503684578
- 0.012024362465437454
- 0.011954388321860682
- 0.012036881480318836
- 0.011892602295626569
- 0.011894659304260454
- 0.011936900217699099
- 0.01196088781580329
- 0.011944285417093506
- 0.012131033807144136
- 0.011920132538704555
- 0.01171758951902201
- 0.012106664997490146
- 0.011835251174442753
- 0.011860985627185695
- 0.011835744981712934
- 0.011828713478591246
- 0.01211762792560496
- 0.01184578631898459
- 0.011647068140910396
- 0.011823394665895383
- 0.012094072856103318
- 0.012009661179035902
- 0.011660150617738313
- 0.011822412715918279
- 0.011739240464153169
- 0.011908366680852597
- 0.011861523436500302
- 0.011792397409488883
- 0.011782847963792236
- 0.011784194113946036
- 0.011847617641044191
- 0.01200240301675623
- 0.011752249413653265
- 0.011889110272160814
- 0.011809706900127327
- 0.011754352145510006
- 0.011816216438207069
- 0.011741992345527757
- 0.011920035722417921
- 0.01172059357071979
- 0.011766950328704676
- 0.011761015292870092
- 0.011791478005485445
- 0.011616910507171592
- 0.011707950324480292
- 0.01165470282865476
- 0.011797678503620474
- 0.01178839356532391
- 0.011725612886580108
- 0.011778749749536
- 0.01173710950968575
- 0.011683099514132813
- 0.011707017345566161
- 0.011718561312867493
- 0.011679273511318467
- 0.01168207480583953
- 0.01179631178184778
- 0.011672459118351151
- 0.011722022502482691
- 0.011726604509344207
- 0.011642122647100234
- 0.011716343833817333
- 0.011698579056117731
- 0.011754558987538272
- 0.011770850990580606
- 0.011695707818092425
- 0.01165295941047842
- 0.011656434593510024
- 0.01169495391836272
- 0.011619986300179852
- 0.011740173307495027
- 0.01181758577121964
