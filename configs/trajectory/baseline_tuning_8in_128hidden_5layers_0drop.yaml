name: baseline_tuning_8in_128hidden_5layers_0drop
frames_in: 8
frames_out: 15
layers: 5
hidden_size: 128
dropout: 0.0
loss: MSE loss
optimizer: Adam
learning rate: 0.002
weight decay: 0.0001
epochs: 100
batch_size: 32
train loss:
- 0.037271635948762864
- 0.02329808381633668
- 0.023651005275830438
- 0.02339605145345006
- 0.02369407738877248
- 0.02334013443345888
- 0.023774896830886225
- 0.02334156784500125
- 0.02325190787639799
- 0.023394365417712098
- 0.02311247811192953
- 0.02316776375415959
- 0.02316641218111485
- 0.02325903485187247
- 0.02316108086629759
- 0.023085891516714155
- 0.02311747785233244
- 0.02304808546706468
- 0.02307237544438884
- 0.023096813289802287
- 0.023088442220623735
- 0.023289431381640555
- 0.023061589915541154
- 0.023094813386567787
- 0.023084769249433958
- 0.02317389463887939
- 0.02320732223459437
- 0.023024270947598204
- 0.02305741770829581
- 0.023132150544773175
- 0.02308419857268469
- 0.023045594061288654
- 0.02301780207530607
- 0.0231782353094107
- 0.023039584672903714
- 0.023179500741011733
- 0.022997903908732572
- 0.0232015429889854
- 0.023070561640624758
- 0.023045806520724597
- 0.023082381161519245
- 0.02300195784979983
- 0.02305316481786438
- 0.023087480046515223
- 0.023052900687708884
- 0.02308650385541252
- 0.02304500774067791
- 0.023091973017759716
- 0.02307248521077482
- 0.02304886986466148
- 0.023135867382435105
- 0.023063207740742196
- 0.02302995374827068
- 0.023028167943128303
- 0.02301947494285016
- 0.023112496750169916
- 0.02303196058337447
- 0.023081015418224698
- 0.02310990135456565
- 0.02307957730291388
- 0.023072511853673792
- 0.023090855642870257
- 0.023045457616637025
- 0.023082565881689138
- 0.023042478964109964
- 0.02304931761861979
- 0.023021675717038444
- 0.023017931423043904
- 0.0230774055883477
- 0.023039292308348645
- 0.023073827989305122
- 0.02296461451421433
- 0.022977359486814543
- 0.02307474326719589
- 0.02302068628700851
- 0.02303614368355727
- 0.02304780938297133
- 0.02300661446267291
- 0.023000585339680503
- 0.02306523381531993
- 0.023013308738605884
- 0.023062637570915343
- 0.023007681758343418
- 0.02301372340114056
- 0.023082432608249822
- 0.022990970153220092
- 0.02300517462618366
- 0.023089660402340225
- 0.02309126916188228
- 0.02298416756093502
- 0.023064546039493025
- 0.023037747055575063
- 0.02301652142424372
- 0.023043714782010904
- 0.023030829462635367
- 0.023011542184726348
- 0.02302532713813118
- 0.023035355654981316
- 0.022970806736546227
- 0.02309119770939969
