name: baseline_tuning_8in_128hidden_3layers_40drop
frames_in: 8
frames_out: 15
layers: 3
hidden_size: 128
dropout: 0.4
loss: MSE loss
optimizer: Adam
learning rate: 0.002
weight decay: 0.0001
epochs: 100
batch_size: 32
train loss:
- 0.038039727407637274
- 0.020861476954497114
- 0.018930422802326045
- 0.014937482338068606
- 0.012721398762792727
- 0.011713844606110567
- 0.01105505753827246
- 0.01036366826229835
- 0.010947042710701876
- 0.009935812393934288
- 0.009551917650748658
- 0.009403913959719335
- 0.009844446236479886
- 0.009350970805964516
- 0.009214833101752815
- 0.00929308937909671
- 0.009165153874060776
- 0.009209091875324898
- 0.008951823274263098
- 0.008851151854459997
- 0.008975052646113724
- 0.008865246513882015
- 0.008827784313832092
- 0.008658592323949443
- 0.008569721144305754
- 0.00855711736728119
- 0.008225799715028534
- 0.008108787633525797
- 0.007860832936071519
- 0.007955422277173286
- 0.007873651453682894
- 0.007792523902947012
- 0.007643984058942599
- 0.007427631682845988
- 0.0075817960432341585
- 0.007296595657597991
- 0.007229416404225969
- 0.007147402974152112
- 0.007163771092184359
- 0.007332016979167356
- 0.0072888073929927395
- 0.007297869412157732
- 0.006859898478783007
- 0.007037789372232141
- 0.006891552765582559
- 0.007155903609163022
- 0.00701805016120212
- 0.006753661513587908
- 0.0068987540729768294
- 0.006770211053847135
- 0.006652860902249813
- 0.006769700588870652
- 0.00692891900698784
- 0.006817968615295389
- 0.006654748351230652
- 0.006640987948196221
- 0.0068286809115375895
- 0.006951585199825371
- 0.0066473715514227556
- 0.006726675183524059
- 0.006934120484828194
- 0.006657211154204192
- 0.0066908045164957835
- 0.006479273844934717
- 0.006614676043037561
- 0.006500832854381091
- 0.006669280861940565
- 0.006600509754299552
- 0.006582803652869372
- 0.0066438645796402344
- 0.0065993348938188975
- 0.006477137482312473
- 0.0065807207823507016
- 0.006539566501314881
- 0.006513209099869562
- 0.006454233510443304
- 0.006367613313884675
- 0.006590094346600243
- 0.006496028455940983
- 0.006674573856959992
- 0.0065342983366520726
- 0.0065011095397080046
- 0.006504331918763397
- 0.0064689943955951856
- 0.0065741983737466455
- 0.006704085712804448
- 0.0066002798001458755
- 0.006672274554148316
- 0.0064903847531333
- 0.0065496728749639245
- 0.006343344586040777
- 0.00655437610406853
- 0.0064021873382167725
- 0.006704541156068444
- 0.006543095070350019
- 0.006595235630351154
- 0.0063465859835283665
- 0.006450704228180119
- 0.0063940316226474845
- 0.006397838931224203
