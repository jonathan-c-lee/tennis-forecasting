name: baseline_tuning_8in_64hidden_5layers_40drop
frames_in: 8
frames_out: 15
layers: 5
hidden_size: 64
dropout: 0.4
loss: MSE loss
optimizer: Adam
learning rate: 0.002
weight decay: 0.0001
epochs: 100
batch_size: 32
train loss:
- 0.04358073550311825
- 0.023214007492023934
- 0.023346277236749854
- 0.02332490082547257
- 0.023150267928272864
- 0.023108523126833046
- 0.023249793208286733
- 0.02328801894253945
- 0.023255324156223972
- 0.023215857676312894
- 0.023297064984806732
- 0.023270706233533122
- 0.023313432059521917
- 0.023128558445391776
- 0.02320698021522051
- 0.023363856693046
- 0.023193806197635735
- 0.023176676593720913
- 0.023186291815548002
- 0.023093340069621424
- 0.023157604687104497
- 0.02323530944465082
- 0.023113786514046827
- 0.023242788625103008
- 0.023114235387950002
- 0.02315156329187411
- 0.0231086478297469
- 0.023018166564310653
- 0.02304775821917419
- 0.02304369230059129
- 0.023125684383926513
- 0.023057044943488095
- 0.02318950675286447
- 0.02311464898948428
- 0.023037882663215263
- 0.023048854562677915
- 0.02301125284991687
- 0.023095193979200682
- 0.023134477118242392
- 0.023224723725756513
- 0.023095079450101794
- 0.023124374920808818
- 0.02303206869931538
- 0.023079964792049382
- 0.023107929874353016
- 0.023025174035773248
- 0.023010523341407504
- 0.02312215273799021
- 0.02307283034241652
- 0.023048603211682808
- 0.023009394566658178
- 0.02312809825415098
- 0.023196111657196962
- 0.023178088985666444
- 0.022981366755653033
- 0.023085316761007793
- 0.02307309394207182
- 0.02300935347057596
- 0.02301321878934963
- 0.023103363517247424
- 0.023015535990648633
- 0.023043421568655516
- 0.023122822064199026
- 0.023053841052364698
- 0.023160794643755957
- 0.023057694393622725
- 0.0230819285033813
- 0.023036134134553656
- 0.02308078144405839
- 0.023104684333069416
- 0.02297218995098072
- 0.023041891464609887
- 0.02308355076120624
- 0.023058468688137924
- 0.023025225482503825
- 0.022999196442999418
- 0.0229868354346556
- 0.023188230805570566
- 0.023018666059721873
- 0.023015834320383736
- 0.023045841993504686
- 0.02306602562813065
- 0.02301091384849971
- 0.0230010772548333
- 0.023047156631946564
- 0.02305020165594318
- 0.023063476598243926
- 0.023051262196577802
- 0.022964318306480026
- 0.02303591467251506
- 0.02304061997351767
- 0.022987920531555066
- 0.02300970640646506
- 0.02302607863292664
- 0.02309196092235514
- 0.023025469878051853
- 0.0230313815836665
- 0.02299023893532119
- 0.02303560875073264
- 0.023038563825473
