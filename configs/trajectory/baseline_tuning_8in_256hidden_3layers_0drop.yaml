name: baseline_tuning_8in_256hidden_3layers_0drop
frames_in: 8
frames_out: 15
layers: 3
hidden_size: 256
dropout: 0.0
loss: MSE loss
optimizer: Adam
learning rate: 0.002
weight decay: 0.0001
epochs: 100
batch_size: 32
train loss:
- 0.03875986773143463
- 0.0215744007779639
- 0.019878672765968722
- 0.015531712140935131
- 0.012390617799909808
- 0.011773744138264203
- 0.010335087746711848
- 0.009925479670585711
- 0.009186226096522958
- 0.009240037484589634
- 0.009068479849897984
- 0.008769599897669086
- 0.008768107170309824
- 0.008820623837269937
- 0.008395626869854293
- 0.008374275728069905
- 0.008048314253410583
- 0.008226458466600013
- 0.007851199382515271
- 0.008052565988530463
- 0.008007715745015612
- 0.007826549113032561
- 0.007696621921620792
- 0.0071743675153938275
- 0.007057448068067808
- 0.006639180693162393
- 0.0066184350495710025
- 0.006716928397647187
- 0.006618221579237452
- 0.006454059948460965
- 0.006477176076202075
- 0.006338365541040143
- 0.006272128653488582
- 0.006163601020846186
- 0.006190912577005316
- 0.00637902625942532
- 0.00609177276138452
- 0.006133310812159996
- 0.006187947950314117
- 0.006019013765845684
- 0.006201580272916752
- 0.00608651060021565
- 0.006402648838024728
- 0.0061086019076689885
- 0.006054523489379053
- 0.00600305982110904
- 0.005910463583997533
- 0.006032286011437072
- 0.005978810429997459
- 0.005948157441696223
- 0.005953995770291437
- 0.006171195812028231
- 0.005968691772321546
- 0.005794542529207619
- 0.005936856148764491
- 0.005951033094662089
- 0.0059557452037506085
- 0.0059962457611779625
- 0.005819417577519824
- 0.005957708302060071
- 0.005944728774690553
- 0.005857575003977252
- 0.005871915075739351
- 0.005827548309476881
- 0.005758616672475127
- 0.005816645481140364
- 0.005809147715497809
- 0.005743667468593656
- 0.005925340300921021
- 0.0057198195010895216
- 0.005972244532165836
- 0.005990677849165624
- 0.005923507676188705
- 0.005937093529920978
- 0.00577877141547071
- 0.005662754596552894
- 0.005912539776108122
- 0.005842524047210058
- 0.0059650242269699335
- 0.005917596327188083
- 0.005825817850241556
- 0.005893465815277039
- 0.00563972294212708
- 0.0057308693929233505
- 0.005777703795605634
- 0.005778696641467417
- 0.005644951045183064
- 0.005787446016233556
- 0.0056970511103355435
- 0.005679130568935336
- 0.005714999841926973
- 0.0057164660708012086
- 0.005777671131529385
- 0.005776592742510234
- 0.005792945658764507
- 0.005727706064434746
- 0.005704102159442404
- 0.005632261589428858
- 0.005643731551952189
- 0.005685643679049762
