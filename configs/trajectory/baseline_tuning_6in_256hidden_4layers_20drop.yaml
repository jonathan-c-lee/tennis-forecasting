name: baseline_tuning_6in_256hidden_4layers_20drop
frames_in: 6
frames_out: 15
layers: 4
hidden_size: 256
dropout: 0.2
loss: MSE loss
optimizer: Adam
learning rate: 0.002
weight decay: 0.0001
epochs: 100
batch_size: 32
train loss:
- 0.042131239222362636
- 0.022821978363208473
- 0.019817948085255922
- 0.018740619427990168
- 0.017058092739898712
- 0.01624312438070774
- 0.016255275765433907
- 0.014656767807900905
- 0.013623410044237972
- 0.013292885536793619
- 0.012919946748297662
- 0.01269132662564516
- 0.01266194402705878
- 0.012413422396639362
- 0.012135153985582292
- 0.01220786503981799
- 0.012178622075589373
- 0.0120661701541394
- 0.012079850432928651
- 0.011968945036642253
- 0.012071125174406916
- 0.01204493922414258
- 0.012086738314246758
- 0.01198067949153483
- 0.01198080718750134
- 0.012040265020914376
- 0.012016191711882129
- 0.011773436679504812
- 0.011857455968856812
- 0.011883197590941563
- 0.012045269028749316
- 0.011849321814952418
- 0.011901405884418636
- 0.011828732845606283
- 0.011882576259085908
- 0.011802745825843886
- 0.011650432826718316
- 0.011639668309362606
- 0.012127298337873071
- 0.011579015257302672
- 0.011834921350236982
- 0.011763318337034433
- 0.011847122199833393
- 0.011622432275908067
- 0.01177002206677571
- 0.01159280858701095
- 0.011699358542682604
- 0.011871925322338939
- 0.0117248221533373
- 0.011557805200573057
- 0.011592295800801367
- 0.011706126941135153
- 0.011785060353577136
- 0.01161157947499305
- 0.01165125978877768
- 0.011755235499003902
- 0.011705649620853364
- 0.011490461666835473
- 0.011675853130873293
- 0.011664336739340798
- 0.01150895589380525
- 0.01175601722788997
- 0.011755139892920851
- 0.011763533140765503
- 0.011833234765799717
- 0.01164718177751638
- 0.011674349301028997
- 0.011599208024563268
- 0.01174170890590176
- 0.011704011680558324
- 0.01145413297927007
- 0.011648321215761825
- 0.01155414681416005
- 0.01185731379664503
- 0.011688969895476475
- 0.011632957006804645
- 0.011693700635805725
- 0.011613593337824569
- 0.011587657168274746
- 0.0113630477047991
- 0.011520931945415213
- 0.011745987698668614
- 0.011631525075063109
- 0.011605540639720857
- 0.011721331399166956
- 0.011586114199599252
- 0.011741230450570583
- 0.011535691743483767
- 0.01155985143268481
- 0.011870845779776573
- 0.01156550032319501
- 0.011618408653885127
- 0.011634395836153999
- 0.011641050834441558
- 0.011483220744412392
- 0.01169069663155824
- 0.011601848615100607
- 0.011666437529493123
- 0.011711849685525522
- 0.011795921681914479
