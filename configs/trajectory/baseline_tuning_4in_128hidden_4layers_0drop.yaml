name: baseline_tuning_4in_128hidden_4layers_0drop
frames_in: 4
frames_out: 15
layers: 4
hidden_size: 128
dropout: 0.0
loss: MSE loss
optimizer: Adam
learning rate: 0.002
weight decay: 0.0001
epochs: 100
batch_size: 32
train loss:
- 0.040415506005103206
- 0.022461202248563
- 0.019536813709562943
- 0.0169248264512898
- 0.01643102491895358
- 0.016144868630317995
- 0.01603523018643444
- 0.01583019508347835
- 0.015875390144409956
- 0.01572937909283756
- 0.015686911220351856
- 0.015389750067742519
- 0.015186133748495284
- 0.01482559491040898
- 0.014760232668507981
- 0.01407371933951422
- 0.013167663178418153
- 0.012570174905345028
- 0.012254269034774215
- 0.012005929254501321
- 0.01180013057626324
- 0.011601149259756008
- 0.011611189049335174
- 0.011391101560244957
- 0.011408226064371842
- 0.011565237892446694
- 0.011222353084357801
- 0.011763129371827767
- 0.011387696667907782
- 0.011348956701472217
- 0.011128854133004759
- 0.011192149994319972
- 0.011159436087365504
- 0.011298461190574332
- 0.01141736468438197
- 0.011340460441086763
- 0.011331996322827942
- 0.011193830473923388
- 0.011200425261056718
- 0.011442953824169107
- 0.011089324410774825
- 0.011136325610870565
- 0.01121907793905264
- 0.011095182268799823
- 0.011047422678934203
- 0.01115651913907057
- 0.011175208644550523
- 0.011120147928365586
- 0.011055083951142467
- 0.01112445365683532
- 0.011151714986305178
- 0.011187919432780257
- 0.011043659411370754
- 0.011035340487451098
- 0.011076393123302195
- 0.010908703750896233
- 0.010934167725528464
- 0.011170810456445187
- 0.011192856218527865
- 0.01092365299018077
- 0.010775214521053397
- 0.010960499278879092
- 0.010785566573893582
- 0.010900163298679722
- 0.010929989749220785
- 0.01110434919066819
- 0.010936819563079396
- 0.010807016398757696
- 0.011009727794401071
- 0.010834155632987803
- 0.010858629289784549
- 0.01077962251679029
- 0.010890188098045778
- 0.01086849837138513
- 0.010783213978334342
- 0.010716382477340507
- 0.010860070057305288
- 0.010780368684388605
- 0.01091815522055567
- 0.010829034847793755
- 0.010991214619328579
- 0.010890317505892411
- 0.01079193357217275
- 0.010885526730828448
- 0.010714484430435631
- 0.010831839910903831
- 0.010777552876575494
- 0.010871061353863757
- 0.010741088591102097
- 0.010857954506336906
- 0.010884085411412848
- 0.011088682246612913
- 0.01074332858286338
- 0.010869377930821092
- 0.010800844547051339
- 0.010854448421777766
- 0.010886397793383143
- 0.010763045728068662
- 0.010671589057892561
- 0.010788341081574743
