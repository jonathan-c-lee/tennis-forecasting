name: baseline_tuning_8in_64hidden_4layers_0drop
frames_in: 8
frames_out: 15
layers: 4
hidden_size: 64
dropout: 0.0
loss: MSE loss
optimizer: Adam
learning rate: 0.002
weight decay: 0.0001
epochs: 100
batch_size: 32
train loss:
- 0.04784995781941504
- 0.02307482154544773
- 0.02324313498278962
- 0.021381901529959484
- 0.01928003905694696
- 0.01748934853821993
- 0.015540378722303275
- 0.013292087053384962
- 0.012766953923185415
- 0.012064413863080966
- 0.012241748079091688
- 0.011912466734176196
- 0.011863422980885717
- 0.011676145316679267
- 0.011742172969057213
- 0.011882698564212533
- 0.011712678336785942
- 0.011902617237566016
- 0.011508504140980636
- 0.011566031282131054
- 0.011666596254121654
- 0.011432896633455647
- 0.011388507901537644
- 0.011388098302332662
- 0.011689489996178618
- 0.011344668726566472
- 0.011207799527416878
- 0.011304809281578924
- 0.01124444437107142
- 0.011251087094173778
- 0.011243984232880647
- 0.011428702021418493
- 0.011288929942853843
- 0.011170409743591576
- 0.011343465952933588
- 0.01109666028944186
- 0.011146498425521805
- 0.011075564383092937
- 0.011186565555443492
- 0.011033316938607375
- 0.011043038307630185
- 0.011047251192452033
- 0.01105844512393203
- 0.011120410500495117
- 0.011211822298650123
- 0.011117998522388029
- 0.011095916504582649
- 0.010988673814159782
- 0.011081990933257945
- 0.010964092515597615
- 0.010998366813210748
- 0.01118395086731526
- 0.010953673927844325
- 0.010937754919493123
- 0.011028875048637768
- 0.0109972584568247
- 0.011052273373013432
- 0.011023024032246085
- 0.010878959181423805
- 0.010897914183479322
- 0.010932905334224807
- 0.010939262294552372
- 0.010957565217550043
- 0.010977357443233457
- 0.01092103265131576
- 0.010824554299204788
- 0.010774021402378625
- 0.010925882772978726
- 0.010921380895225308
- 0.010811398972910417
- 0.01102082173729056
- 0.010931135774177463
- 0.010871876467349408
- 0.010767894649665944
- 0.010899611389193731
- 0.010795441621183595
- 0.01091133145898392
- 0.01081444974988699
- 0.0109323534312882
- 0.010846589397214636
- 0.010799111096966492
- 0.010880321246583629
- 0.010963480972649552
- 0.010722651028463358
- 0.01076627605229239
- 0.010817508289849833
- 0.010679356653620548
- 0.01070487847598859
- 0.010860847934043105
- 0.010775381522371045
- 0.010749266235322892
- 0.010756326002316385
- 0.010806842200175117
- 0.010706670546927784
- 0.010811074825498876
- 0.010719941345432514
- 0.010792303612290681
- 0.010843008509070813
- 0.010769247643272334
- 0.01071912279541168
