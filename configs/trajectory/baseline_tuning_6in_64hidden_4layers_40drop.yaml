name: baseline_tuning_6in_64hidden_4layers_40drop
frames_in: 6
frames_out: 15
layers: 4
hidden_size: 64
dropout: 0.4
loss: MSE loss
optimizer: Adam
learning rate: 0.002
weight decay: 0.0001
epochs: 100
batch_size: 32
train loss:
- 0.04863305785693228
- 0.02338001406751573
- 0.021982886153273283
- 0.020227018173318356
- 0.019738915748894213
- 0.01770715758902952
- 0.017360317031852902
- 0.016688065289054067
- 0.016054634493775664
- 0.01519834272330627
- 0.013706646510399878
- 0.013407533417921513
- 0.013149276201147586
- 0.013009536464232951
- 0.013162679166998714
- 0.013079716521315277
- 0.013003286137245596
- 0.012623532104771584
- 0.012559876451268792
- 0.01256768642924726
- 0.012243265949655325
- 0.012254268012475223
- 0.012140527268638835
- 0.012078137637581676
- 0.012070700066396966
- 0.012030693551059813
- 0.012084307591430843
- 0.012232774635776877
- 0.011905382934492082
- 0.012175296025816351
- 0.011978379980428144
- 0.011922350968234241
- 0.012018047593301162
- 0.012057469820138066
- 0.011855525424471125
- 0.011842402233742177
- 0.01209503598511219
- 0.011838797992095352
- 0.011871292325668036
- 0.01161054169642739
- 0.01186034616548568
- 0.011824915232136846
- 0.0118296614557039
- 0.011862098961137236
- 0.011710623139515519
- 0.011830099252983927
- 0.011756054998841137
- 0.011732617812231182
- 0.011961880896706134
- 0.011765534372534603
- 0.012006560829468072
- 0.011771907180082053
- 0.011636278429068624
- 0.011715986271155998
- 0.011598833976313472
- 0.011742846708511934
- 0.011642666556872428
- 0.011670559458434582
- 0.011700424348236992
- 0.011658600578084589
- 0.011739358713384718
- 0.011901156953535974
- 0.01170237967162393
- 0.01175209020730108
- 0.011642692860914395
- 0.011785523657454177
- 0.011660831148037686
- 0.01156783422920853
- 0.011730985186295584
- 0.011673847655765712
- 0.01156639758264646
- 0.011722241947427391
- 0.01188507509068586
- 0.011570391594432294
- 0.01169098080135882
- 0.011923699453473092
- 0.011633840610738844
- 0.011596504296176136
- 0.011629543441813439
- 0.01172739354078658
- 0.01162084488896653
- 0.011533934675389901
- 0.01164588057436049
- 0.011787726939655841
- 0.011463949724566191
- 0.011586378363426774
- 0.011777144263032824
- 0.011488563165767119
- 0.011526583397062496
- 0.011484719591680913
- 0.011668895883485676
- 0.011492008931236341
- 0.011594491370487958
- 0.011560716596432031
- 0.01149007553467527
- 0.011671673861565068
- 0.011448218236910179
- 0.011616832937579601
- 0.01168699044501409
- 0.011512285808566957
