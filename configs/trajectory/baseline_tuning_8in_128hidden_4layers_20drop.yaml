name: baseline_tuning_8in_128hidden_4layers_20drop
frames_in: 8
frames_out: 15
layers: 4
hidden_size: 128
dropout: 0.2
loss: MSE loss
optimizer: Adam
learning rate: 0.002
weight decay: 0.0001
epochs: 100
batch_size: 32
train loss:
- 0.03667648318164711
- 0.02188198023204562
- 0.02050387294656491
- 0.0184947310039137
- 0.016683423561574536
- 0.01515534977558293
- 0.013809105322425123
- 0.013031689845072695
- 0.012913641519844532
- 0.012511519361523133
- 0.012418731286697373
- 0.012525118415868735
- 0.012662582417737834
- 0.012485227795152725
- 0.012475400673862122
- 0.012171526097610026
- 0.01223535765599035
- 0.012234459961234014
- 0.012151564353534693
- 0.012039153211856191
- 0.012182463891804218
- 0.011931810557512166
- 0.01223908989584144
- 0.012027907421034349
- 0.012103283577421797
- 0.0119116523692125
- 0.011889103788269472
- 0.011969244355170787
- 0.012006101728994635
- 0.011860998441712766
- 0.011818604596855142
- 0.011856004536812063
- 0.011799410398153565
- 0.012051539312859502
- 0.01183393360646088
- 0.011833505109659855
- 0.011848772198103274
- 0.011970424405711739
- 0.011755645504999386
- 0.011816365066679973
- 0.0117400674373383
- 0.011641998787092257
- 0.011925535535887826
- 0.011828465734997505
- 0.011774619127611947
- 0.011724383358053769
- 0.011597769570690168
- 0.011793880923827993
- 0.011732286726466463
- 0.011605421116552021
- 0.011855367406070986
- 0.011796000460752203
- 0.011870612732217282
- 0.01165747706177114
- 0.011623710388011193
- 0.011498089685234466
- 0.011574860450021828
- 0.011569616390698695
- 0.011702948986682334
- 0.011847401780512514
- 0.011693667284436996
- 0.011656263418778588
- 0.011642312554383203
- 0.01170389032392185
- 0.011627669823414918
- 0.011728838451583928
- 0.01145527838387444
- 0.011714316197211229
- 0.0116298370051516
- 0.011741409714840636
- 0.011887759524338607
- 0.011690320948115255
- 0.011463318603655582
- 0.011582225584719755
- 0.01162264015690624
- 0.01154498625075138
- 0.011512235691181467
- 0.011464422615834429
- 0.011823366225190177
- 0.011603741240510834
- 0.011560067404768889
- 0.011609702689360969
- 0.01164466021818262
- 0.011761613649940944
- 0.011692757541431656
- 0.011642934966690933
- 0.011588691038232816
- 0.011531552223229333
- 0.011640731068445912
- 0.011872049043827418
- 0.011648808712068992
- 0.011737003221023308
- 0.011645447403852698
- 0.011643583685914172
- 0.01172140665069411
- 0.01152645185283279
- 0.011730512568750713
- 0.011594674650202447
- 0.011700868182167222
- 0.011608613489926615
