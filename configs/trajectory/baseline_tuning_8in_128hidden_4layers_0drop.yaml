name: baseline_tuning_8in_128hidden_4layers_0drop
frames_in: 8
frames_out: 15
layers: 4
hidden_size: 128
dropout: 0.0
loss: MSE loss
optimizer: Adam
learning rate: 0.002
weight decay: 0.0001
epochs: 100
batch_size: 32
train loss:
- 0.037037567176584955
- 0.022556069047673592
- 0.020355570035739037
- 0.01867354406586176
- 0.016714810110817227
- 0.014053303925202617
- 0.013173318833489961
- 0.012683179536009137
- 0.012545954471430447
- 0.012258225321015226
- 0.012133437751120404
- 0.01182651052935214
- 0.011928465441340887
- 0.011868513118522831
- 0.011737853335806086
- 0.011783559983478317
- 0.012105218099453781
- 0.011856288531252855
- 0.011502333934407067
- 0.011601491277165052
- 0.011651605308716055
- 0.011453034592014326
- 0.01158053133212313
- 0.011356186764196882
- 0.011488471846273052
- 0.011344344166543665
- 0.011398818939216907
- 0.011422391261763965
- 0.011376381521643717
- 0.011401515389214965
- 0.01133055230484733
- 0.011268499731734584
- 0.011275500029655574
- 0.011566284336621248
- 0.01133849838442063
- 0.011214315030676655
- 0.011173179384933996
- 0.011244626250118017
- 0.011173569013753647
- 0.0114278933269125
- 0.011293312835306683
- 0.011172173650864559
- 0.011262260059106953
- 0.011057944484998155
- 0.011181129331264314
- 0.011112280785330112
- 0.011101771234475736
- 0.011045423383482649
- 0.011130814711692968
- 0.011202669166075655
- 0.01105723917319239
- 0.011037684747171176
- 0.011138760284343852
- 0.010971184307119892
- 0.01120231770969267
- 0.011061947963707432
- 0.011029669065971541
- 0.011091696133813526
- 0.011016521655919053
- 0.011083949551787934
- 0.011137495624915332
- 0.011113132715602464
- 0.010937911906291413
- 0.010981863788977454
- 0.011272175443842064
- 0.010952941130233717
- 0.010949839914476947
- 0.010957390246794948
- 0.011049914368298611
- 0.010946089342896696
- 0.010890260550983345
- 0.0109214675023303
- 0.011135244299952366
- 0.010937729237388961
- 0.011018234758956146
- 0.011040930618543792
- 0.010971741904091985
- 0.010926533231063735
- 0.010809570786696447
- 0.01089879730126903
- 0.0109897176086714
- 0.010996889311207247
- 0.010953225649280262
- 0.010883520292471858
- 0.010833904742487247
- 0.01083399696699992
- 0.010875050261428085
- 0.01090523554601624
- 0.01086309523899344
- 0.010964037726715774
- 0.0109088674211238
- 0.010751630403573
- 0.010943574824995255
- 0.010791469736708493
- 0.010838523601429373
- 0.010874593707060889
- 0.01083222690592461
- 0.010809309179364126
- 0.010833913277646032
- 0.010819155793589882
