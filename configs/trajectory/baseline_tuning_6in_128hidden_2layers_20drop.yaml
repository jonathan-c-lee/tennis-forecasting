name: baseline_tuning_6in_128hidden_2layers_20drop
frames_in: 6
frames_out: 15
layers: 2
hidden_size: 128
dropout: 0.2
loss: MSE loss
optimizer: Adam
learning rate: 0.002
weight decay: 0.0001
epochs: 100
batch_size: 32
train loss:
- 0.03651542147854343
- 0.01859359125373885
- 0.014357772201765328
- 0.013301072386093437
- 0.012597345269750804
- 0.0114988032146357
- 0.010416871740017087
- 0.010001499950885773
- 0.00932248699828051
- 0.009321446536341682
- 0.008864327589981258
- 0.008743835776112974
- 0.008727759181056172
- 0.00846977434703149
- 0.008551196707412601
- 0.008256701641948893
- 0.008607897133333608
- 0.008347986079752446
- 0.008117701765149832
- 0.00818825694732368
- 0.008018899784656242
- 0.008330498595023527
- 0.008155876019736752
- 0.007969710259931161
- 0.0078804679622408
- 0.00810879078344442
- 0.007874453387921676
- 0.008033839490963146
- 0.0077389321872033175
- 0.007810834998963401
- 0.0075821075239218775
- 0.007475575507851317
- 0.007676362514030188
- 0.007588761713122949
- 0.007435858889948577
- 0.0073781687999144195
- 0.007410427852300927
- 0.007301764044677838
- 0.007318392273737118
- 0.007414063136093318
- 0.0071981556306127455
- 0.0072365005617029965
- 0.0070494776533450935
- 0.007020582712721079
- 0.006999288691440597
- 0.006784514553146437
- 0.006645841646241024
- 0.006761026487220078
- 0.006722185600665398
- 0.006494358245981857
- 0.006274935530382208
- 0.00635636459919624
- 0.00631202693330124
- 0.006330238247755915
- 0.006409143429482356
- 0.006307847125572153
- 0.006355000764597207
- 0.006355717167025432
- 0.0063170469540636985
- 0.006410172389587387
- 0.0064656640170142055
- 0.006228998067672365
- 0.006367223430424928
- 0.00621530304197222
- 0.0062701448216103016
- 0.006275580765213817
- 0.006145639234455302
- 0.006270101433619857
- 0.006113260640995577
- 0.006129846221301705
- 0.00627684011706151
- 0.0060793337121140215
- 0.006054534271243028
- 0.0062394057109486315
- 0.0060734998842235655
- 0.006151062625576742
- 0.0060777835955377665
- 0.006058721529552713
- 0.00611031326407101
- 0.00607429668889381
- 0.006118113987031393
- 0.005976927152369171
- 0.005995986293419265
- 0.006240986048942432
- 0.00608062875980977
- 0.0061427773558534685
- 0.006123145218589343
- 0.0060153929749503735
- 0.006048939877655357
- 0.006067898665787652
- 0.006108993571251631
- 0.00598083168733865
- 0.005975598524673842
- 0.006208921974757686
- 0.006049185615847818
- 0.006255108673940413
- 0.00588745123532135
- 0.00612698863260448
- 0.005920702836010605
- 0.006046313405386172
