name: baseline_tuning_4in_256hidden_4layers_40drop
frames_in: 4
frames_out: 15
layers: 4
hidden_size: 256
dropout: 0.4
loss: MSE loss
optimizer: Adam
learning rate: 0.002
weight decay: 0.0001
epochs: 100
batch_size: 32
train loss:
- 0.03742207485584565
- 0.023815001832482256
- 0.0208130911981434
- 0.017134022536791034
- 0.016642921730692005
- 0.016558173951911337
- 0.01646309949595619
- 0.016338711546986927
- 0.016504558447150538
- 0.016687889755875977
- 0.015707543257761886
- 0.016016279105786926
- 0.01571406447041182
- 0.01588443627603996
- 0.015914200736140763
- 0.015786752633658457
- 0.015600013466160974
- 0.015609482303261757
- 0.0154012197766591
- 0.015337650629657286
- 0.015134233969873116
- 0.01511876213789722
- 0.014831050655540124
- 0.014215280382353583
- 0.01396630980350353
- 0.01375959398523893
- 0.014102988611952758
- 0.01381713698077717
- 0.013228556083768238
- 0.013043728323630344
- 0.0131021450492151
- 0.01334118533787536
- 0.012823033261538289
- 0.01284772480757516
- 0.01300926628396099
- 0.012912597512987293
- 0.012775133927295238
- 0.012659491746927853
- 0.012762412221895324
- 0.012591312374965644
- 0.012437705634690729
- 0.012524878997125743
- 0.012713273803209081
- 0.012511326505998035
- 0.012641060361523687
- 0.012515737916584368
- 0.012428970283104314
- 0.012321721428982269
- 0.012399380509224203
- 0.01271242134411026
- 0.012289679189569053
- 0.012531697146456551
- 0.012384084800695196
- 0.012399406546013958
- 0.01221416268589688
- 0.012557730297155586
- 0.012676336595581638
- 0.012626830729897376
- 0.01229000680240584
- 0.012253389239633157
- 0.01237113075905744
- 0.012372116592747194
- 0.012209649324233149
- 0.012556219186035939
- 0.01269325705383111
- 0.012282486694554487
- 0.01213233860462536
- 0.012601219025658972
- 0.01260257692846619
- 0.012433981660891462
- 0.01215714537596077
- 0.012280038822396302
- 0.012401528277054982
- 0.01222477726822282
- 0.012557445358438992
- 0.012536776378566835
- 0.012018367504402076
- 0.012110804017909147
- 0.012061574185887972
- 0.012202910527035041
- 0.012264157582166386
- 0.012307136078123693
- 0.012469908239802829
- 0.012323757374866141
- 0.012291852154849488
- 0.01212096970850303
- 0.012487532616949376
- 0.012383976468333491
- 0.012153813327996083
- 0.012096457861731818
- 0.012213589485596728
- 0.012283974154679864
- 0.012189925027390322
- 0.012336009446485543
- 0.012314459268739562
- 0.01222557322708545
- 0.011990861421060046
- 0.012119539179781706
- 0.01229359357284359
- 0.012033785486386882
