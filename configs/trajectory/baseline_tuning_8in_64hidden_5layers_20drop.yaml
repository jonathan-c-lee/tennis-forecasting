name: baseline_tuning_8in_64hidden_5layers_20drop
frames_in: 8
frames_out: 15
layers: 5
hidden_size: 64
dropout: 0.2
loss: MSE loss
optimizer: Adam
learning rate: 0.002
weight decay: 0.0001
epochs: 100
batch_size: 32
train loss:
- 0.04638114859220348
- 0.023480946315994747
- 0.02360203138352195
- 0.02338734712404541
- 0.023388723489132863
- 0.02342108134887641
- 0.023328996416700037
- 0.023470585977163495
- 0.023185312406077414
- 0.023439922381805468
- 0.023437049876474127
- 0.023183490350087987
- 0.023127108293620847
- 0.022999287771556198
- 0.023103388085301164
- 0.023079856334230566
- 0.023161613040521174
- 0.023071085609778573
- 0.02304552470722908
- 0.02311875145363657
- 0.023189073947222926
- 0.023059179416940183
- 0.023068370097139967
- 0.023166112365979184
- 0.023104044903494134
- 0.023061719145390052
- 0.023030271682935425
- 0.023078232461327237
- 0.02304661003990641
- 0.023008896969258785
- 0.02306233529190097
- 0.023137676258441767
- 0.02303797447511667
- 0.023034799407837512
- 0.023132343517168412
- 0.023061995889661433
- 0.02303906299079521
- 0.023101320112996464
- 0.023063896553991717
- 0.023077201345770418
- 0.023174602196469336
- 0.023046190767914435
- 0.023143556546655637
- 0.023034442864547047
- 0.023102459085138537
- 0.023018908062124553
- 0.023047467941253244
- 0.023102003574088405
- 0.023044470815530307
- 0.023017233143313023
- 0.023038829110940046
- 0.02305862835690945
- 0.02304239971916887
- 0.023073107263521304
- 0.023093511798430846
- 0.023036626698095586
- 0.0230753650959534
- 0.023096976448086243
- 0.023097385169018672
- 0.02308582310576605
- 0.023040524153392528
- 0.023049591875434677
- 0.023015845154376723
- 0.02302370338311678
- 0.02302021674717529
- 0.0230148939910946
- 0.023112690972187853
- 0.023008306428224226
- 0.023026080589882935
- 0.023033404675654217
- 0.02296646568877033
- 0.023014334360538406
- 0.02303778763294597
- 0.023105270441480075
- 0.023057651151961917
- 0.02313378101945678
- 0.023030226578629468
- 0.022986115781661078
- 0.023069230792454525
- 0.022972147298764577
- 0.023005062171929998
- 0.023003984006899823
- 0.022989958571859554
- 0.0230191839811734
- 0.022987026485461224
- 0.02298568954243313
- 0.022982493196201475
- 0.023095355133372773
- 0.023001338272720953
- 0.023052967188856268
- 0.023026827970354616
- 0.022977077024929887
- 0.02303710667109942
- 0.02300115102972788
- 0.02300013993171197
- 0.02303669295167621
- 0.02300669143235759
- 0.02296026934032576
- 0.02301380253998162
- 0.022996376586865774
