name: baseline_tuning_4in_64hidden_4layers_0drop
frames_in: 4
frames_out: 15
layers: 4
hidden_size: 64
dropout: 0.0
loss: MSE loss
optimizer: Adam
learning rate: 0.002
weight decay: 0.0001
epochs: 100
batch_size: 32
train loss:
- 0.04674133166302869
- 0.022874567266783597
- 0.020381784549465886
- 0.017628209780395768
- 0.016641557515587335
- 0.016291848763271614
- 0.016415493240878907
- 0.016033140482541956
- 0.01591744680923444
- 0.015841284686308583
- 0.015950836114769364
- 0.01569656532165813
- 0.015512307967852664
- 0.01523243847452564
- 0.015249454416334629
- 0.014820740512215796
- 0.01424375508716445
- 0.013239704141462291
- 0.012348216826119173
- 0.012250707657248885
- 0.012281066191923103
- 0.011655325299979728
- 0.011807529721409082
- 0.011629912875776674
- 0.01183038178463409
- 0.011329089113177709
- 0.011290096493874803
- 0.011307538693977727
- 0.0112121331470984
- 0.011233478974275015
- 0.01122771397256005
- 0.01137158716166461
- 0.011154702697869069
- 0.011107448091799463
- 0.011314748326477445
- 0.011188193086396766
- 0.011045793255354151
- 0.011189759208786267
- 0.010990492643121585
- 0.010955990504841378
- 0.01094223523259531
- 0.011029134895422576
- 0.011248162441691498
- 0.011114650998080585
- 0.010906856041401625
- 0.011388949135801307
- 0.011059006917531844
- 0.010937651038675764
- 0.010965652809834775
- 0.011081447784593444
- 0.01101050033222562
- 0.010889317892077897
- 0.011117235515956525
- 0.01088874257043188
- 0.010958370942346475
- 0.010835335245011029
- 0.011049874247638163
- 0.011050642278321364
- 0.010964563996013667
- 0.010943357734998434
- 0.010835204957581964
- 0.01084508919329555
- 0.010844973450595582
- 0.01088148610735381
- 0.010761205118471458
- 0.011036461178949217
- 0.010863981646612102
- 0.010841836445723419
- 0.010758517269780974
- 0.0108781207272392
- 0.010836788182963191
- 0.010995555991007959
- 0.01105383734713182
- 0.010878363854171317
- 0.010738963335982444
- 0.011032794412501433
- 0.010880978513555026
- 0.010733688112210345
- 0.010714786380161474
- 0.010725447396391335
- 0.010818361931330995
- 0.010822337937115886
- 0.010703843937795839
- 0.010684974986002033
- 0.010741288279309685
- 0.010812818342934788
- 0.010830723502339773
- 0.010808404713271208
- 0.010612502602147467
- 0.010662987512064936
- 0.010665601677042835
- 0.010529437849735037
- 0.010577596810266928
- 0.010671398556434813
- 0.010667600726455818
- 0.01060020038858056
- 0.010547682568200944
- 0.010520780476293078
- 0.010573814318365888
- 0.010552931743685478
