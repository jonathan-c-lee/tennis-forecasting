name: baseline_tuning_8in_256hidden_2layers_0drop
frames_in: 8
frames_out: 15
layers: 2
hidden_size: 256
dropout: 0.0
loss: MSE loss
optimizer: Adam
learning rate: 0.002
weight decay: 0.0001
epochs: 100
batch_size: 32
train loss:
- 0.03756179438927506
- 0.019253480354253248
- 0.014349121234933787
- 0.01261735343244634
- 0.010334316399419986
- 0.009669552409809224
- 0.009673843219216112
- 0.0091603538493001
- 0.008979729482833343
- 0.00856629974220561
- 0.008660196680339832
- 0.008576434223523622
- 0.008219031222117475
- 0.008546888751649781
- 0.008437936229607727
- 0.008042424746282115
- 0.007848706898055498
- 0.007804976045330868
- 0.007738355606134179
- 0.0073250181548580335
- 0.006995213904736351
- 0.006853697526775584
- 0.006438369090562757
- 0.006426540518745403
- 0.006386587384475183
- 0.00647044569973044
- 0.0063449778959532325
- 0.006164168452702557
- 0.006202337090397561
- 0.00623451529624813
- 0.006056111161538129
- 0.006105076306487752
- 0.006304533241458143
- 0.005993301703158436
- 0.006231898397702394
- 0.005995464440482327
- 0.006183062621121165
- 0.005982619940150011
- 0.006430345619851841
- 0.005903263971256682
- 0.005958289064207598
- 0.005884020833911587
- 0.005916910658625862
- 0.005999907954371994
- 0.006141300725785992
- 0.005911741532456083
- 0.005936737440502908
- 0.006013712150197995
- 0.005858321096512336
- 0.0058076816605097505
- 0.00596440400150192
- 0.005956618940528435
- 0.005931978211207669
- 0.006012114911351966
- 0.005951545186505853
- 0.0060701931302164554
- 0.005994699255834463
- 0.005947186697062247
- 0.0060766689667973335
- 0.005779448905488169
- 0.005872375461496884
- 0.005694797150554914
- 0.005873938447713286
- 0.005866183642934584
- 0.005735110523933663
- 0.005837160625341761
- 0.0056775457202162165
- 0.005699886577911204
- 0.0056854404178978525
- 0.005850207151939409
- 0.0057660710314123695
- 0.005724322581310061
- 0.005842402238469524
- 0.005613701264786569
- 0.00570594686484318
- 0.005771883221083804
- 0.00553107572589777
- 0.005727673630241918
- 0.0056039950562806065
- 0.005649584534141837
- 0.005532986163220639
- 0.005419675022057152
- 0.005457327981137588
- 0.005448774920917953
- 0.005503464389969653
- 0.00550621702878064
- 0.0053610836356123796
- 0.005298725960045298
- 0.005386918099432052
- 0.005426544760931519
- 0.005407260598237568
- 0.00559440474143794
- 0.005388184182369445
- 0.005353159176869483
- 0.005272144277851227
- 0.005538984920971001
- 0.005384664652468283
- 0.005332376666461365
- 0.005356536217363952
- 0.0052822439082532745
