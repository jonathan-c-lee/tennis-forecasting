name: baseline_tuning_4in_256hidden_5layers_20drop
frames_in: 4
frames_out: 15
layers: 5
hidden_size: 256
dropout: 0.2
loss: MSE loss
optimizer: Adam
learning rate: 0.002
weight decay: 0.0001
epochs: 100
batch_size: 32
train loss:
- 0.03569728372917499
- 0.023643843300732565
- 0.023697715735545865
- 0.023708735776628242
- 0.02359607731808483
- 0.023488560024602912
- 0.02353800419304106
- 0.023253228952303345
- 0.023532941810970688
- 0.02354603079090148
- 0.023558085140438726
- 0.023547327191925344
- 0.023414446972310543
- 0.02347085907779358
- 0.023400387715226338
- 0.023396133192251495
- 0.02331608053250813
- 0.02345317781523422
- 0.023318194174840125
- 0.02341021143221929
- 0.023206725946547074
- 0.023338752189352188
- 0.02333419422768516
- 0.02316928355966085
- 0.023415776268567567
- 0.02329909629016011
- 0.023362166524209357
- 0.023225917411899125
- 0.02346714006529914
- 0.023233652057379116
- 0.02326968803219957
- 0.023258637130996327
- 0.023230419160775197
- 0.02366702481644389
- 0.023292873790602624
- 0.023231029832436714
- 0.023299863607979115
- 0.023182294526953758
- 0.02308576009063809
- 0.023214322664303545
- 0.02346206029854071
- 0.02328802203690564
- 0.023243497620210236
- 0.023357375076523534
- 0.023241130382190518
- 0.02354154919768557
- 0.02333190048366417
- 0.023234572330558742
- 0.023146616386962526
- 0.02321351259762858
- 0.023307177054201378
- 0.02329949635644386
- 0.0233935028038643
- 0.02329342607638718
- 0.02334928835661691
- 0.023365308956047635
- 0.023357576909072607
- 0.02339124541591715
- 0.023151344757665088
- 0.023138047954826442
- 0.023165659772025213
- 0.02319714107355218
- 0.02330734369195538
- 0.023127410445868232
- 0.023171879121182876
- 0.023326920414412464
- 0.023339001254902944
- 0.023314655723947066
- 0.023148526368593728
- 0.023426215095376526
- 0.02324322268458796
- 0.023225666322733884
- 0.02309076539758179
- 0.023239341162053156
- 0.023082126846054086
- 0.02315849844182347
- 0.023213566660329147
- 0.023129564112075316
- 0.023172562895917598
- 0.02307937357482719
- 0.023189817164323213
- 0.02319154406633274
- 0.023349702772166993
- 0.02331073341868174
- 0.02331090581683833
- 0.023270007165401806
- 0.023264002077925353
- 0.023175109764216124
- 0.023221487846271493
- 0.023218104546820675
- 0.023301803104487467
- 0.023251117977463168
- 0.02317997854616907
- 0.023199605169119657
- 0.023385334932417783
- 0.023222897069551694
- 0.023199272813436426
- 0.023229433522548203
- 0.023357364498538737
- 0.02313326279644245
