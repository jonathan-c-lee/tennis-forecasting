name: baseline_tuning_8in_128hidden_2layers_20drop
frames_in: 8
frames_out: 15
layers: 2
hidden_size: 128
dropout: 0.2
loss: MSE loss
optimizer: Adam
learning rate: 0.002
weight decay: 0.0001
epochs: 100
batch_size: 32
train loss:
- 0.03665792741635932
- 0.020258195851516874
- 0.017655078038761887
- 0.013905419083901598
- 0.012384469172905517
- 0.010556313327124601
- 0.009758918510773514
- 0.009630733399498688
- 0.009189887720902887
- 0.009253733926983197
- 0.008870977531128291
- 0.008722161031269196
- 0.008532760629453991
- 0.008415857324070191
- 0.00838282459144351
- 0.00842750524583308
- 0.008298966968785735
- 0.008087798086572675
- 0.008019296717653168
- 0.007875546860166742
- 0.007695467417611729
- 0.0077387832364515415
- 0.007806833951344973
- 0.007243701433645019
- 0.007084813564308459
- 0.007238125210462869
- 0.006906883855927972
- 0.006867980431246606
- 0.006607988731393331
- 0.006634116284785014
- 0.006544728327212455
- 0.006522865683028969
- 0.006356058282706934
- 0.006331169232136652
- 0.006377652570416656
- 0.006473378773448588
- 0.006206988675307624
- 0.006246323927152383
- 0.006460556784008122
- 0.0060343960052521165
- 0.0061269400440910945
- 0.006311014713130042
- 0.006179628540066224
- 0.006108037573344345
- 0.006276065103497498
- 0.006139635544600366
- 0.006102550096951331
- 0.006325090931775638
- 0.006227755247131933
- 0.006098233074940081
- 0.006054413737729192
- 0.005961967564388355
- 0.0062636672165480595
- 0.006076435095783842
- 0.006008692327415264
- 0.00609758558176175
- 0.006130869562230721
- 0.006027336226469731
- 0.006178674202624567
- 0.006236979449049959
- 0.006156857717852992
- 0.006040652877875144
- 0.0060269492442566385
- 0.006260139720302217
- 0.006227120648055703
- 0.006235154139325965
- 0.006023098322668973
- 0.006126042873940513
- 0.006157386169733503
- 0.006092792739384348
- 0.006135928353200419
- 0.006051391861224665
- 0.006106306300675378
- 0.006145954641949715
- 0.006202546732287996
- 0.006160437833824301
- 0.006157992581565735
- 0.006024368643713526
- 0.006214681175784974
- 0.006178273394986798
- 0.006166437564158364
- 0.006108153679207722
- 0.006078269079186117
- 0.006077599337418811
- 0.005922465812108374
- 0.00608059548171638
- 0.006022763691984023
- 0.006151937829519186
- 0.005954234592640136
- 0.005949406336427112
- 0.006012509765598593
- 0.006064630284481034
- 0.006016036925860975
- 0.005985093185135835
- 0.006040610505645222
- 0.0060763537612614
- 0.006068309353475895
- 0.006051036458509632
- 0.006022521486222933
- 0.006036376011781866
