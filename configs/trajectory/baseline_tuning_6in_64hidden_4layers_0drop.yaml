name: baseline_tuning_6in_64hidden_4layers_0drop
frames_in: 6
frames_out: 15
layers: 4
hidden_size: 64
dropout: 0.0
loss: MSE loss
optimizer: Adam
learning rate: 0.002
weight decay: 0.0001
epochs: 100
batch_size: 32
train loss:
- 0.04409187021665275
- 0.02308424129150808
- 0.0208880421356298
- 0.01865534675307572
- 0.01737912363605574
- 0.017225202813278884
- 0.01611184058710933
- 0.015005216235294938
- 0.013269949122332036
- 0.012830977072007954
- 0.012385778862517327
- 0.012049081170698629
- 0.012115566851571202
- 0.01180856788996607
- 0.011680156632792205
- 0.01149448860087432
- 0.011601323576178402
- 0.0114390873699449
- 0.011557939427439124
- 0.011397216725163162
- 0.011382744531147182
- 0.011424168024677783
- 0.01168930041603744
- 0.011388760741101577
- 0.01139244723599404
- 0.011138139356626197
- 0.011186967190587893
- 0.011280428030295298
- 0.011134397640125826
- 0.011185575474519283
- 0.011190788785461337
- 0.01117848091525957
- 0.011119711754145101
- 0.011142877739621327
- 0.011204271769383923
- 0.01114457332296297
- 0.011161025642650202
- 0.011102447076700628
- 0.011036286322632805
- 0.010941321682184934
- 0.010917444311780855
- 0.011056237970478833
- 0.011219504231121391
- 0.011036801122827455
- 0.010997179901460185
- 0.011279999010730535
- 0.010951051284791902
- 0.011069845559541136
- 0.01096254475414753
- 0.011014143074862658
- 0.010940532863605768
- 0.011012664949521422
- 0.010935564764076844
- 0.01094487538211979
- 0.011055731284432112
- 0.011045444040792062
- 0.010886663198471069
- 0.011010094592347742
- 0.010830719966907053
- 0.010973583068698644
- 0.010811220336472615
- 0.010825342359021306
- 0.010888247832190246
- 0.010908271343214437
- 0.011160486022708938
- 0.01098749446682632
- 0.010845790564781055
- 0.010811000393005088
- 0.010791773413075134
- 0.010922432661755011
- 0.010828709113411605
- 0.010784607764799147
- 0.01079635084606707
- 0.010948807332897558
- 0.010912407346768304
- 0.010788616730133071
- 0.010942686628550292
- 0.010857764905085786
- 0.010807409032713622
- 0.011156919214408844
- 0.01075284053804353
- 0.01077967947931029
- 0.010821139463223517
- 0.010758621635613964
- 0.010760614473838359
- 0.010853955114725977
- 0.010723473521647976
- 0.010729634715244175
- 0.010688976832898333
- 0.010806410078657791
- 0.010719203087501229
- 0.010807184333680197
- 0.010760972823482007
- 0.010810174187645316
- 0.010753492725780234
- 0.010725528083276004
- 0.010694450477603822
- 0.010763068607775494
- 0.010668951622210442
- 0.010650239029200748
