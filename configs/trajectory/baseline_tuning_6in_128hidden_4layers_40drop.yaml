name: baseline_tuning_6in_128hidden_4layers_40drop
frames_in: 6
frames_out: 15
layers: 4
hidden_size: 128
dropout: 0.4
loss: MSE loss
optimizer: Adam
learning rate: 0.002
weight decay: 0.0001
epochs: 100
batch_size: 32
train loss:
- 0.03935378154274076
- 0.022491734160576017
- 0.0202819591620937
- 0.01866304420400411
- 0.01745423930697143
- 0.01692540997173637
- 0.016456646274309605
- 0.015266543149482458
- 0.014193552371580154
- 0.013554634584579617
- 0.013563316711224616
- 0.01310947312740609
- 0.013069768017157913
- 0.012745692895259708
- 0.012723625800572336
- 0.01288261681329459
- 0.012563097826205193
- 0.01253589935367927
- 0.012694516102783382
- 0.012453043798450381
- 0.012323124229442328
- 0.012568657880183309
- 0.01235364043386653
- 0.01251146116410382
- 0.01248773658880964
- 0.01282160533592105
- 0.012186993286013604
- 0.012284663552418352
- 0.012272692075930536
- 0.012248024804284796
- 0.012361711543053388
- 0.01237264855299145
- 0.012199580762535333
- 0.012212050362722947
- 0.012175286666024476
- 0.012398306094110013
- 0.012119720253394917
- 0.012024700257461518
- 0.012476602563401685
- 0.01233893244061619
- 0.012077980092726648
- 0.012341845454648137
- 0.012261893239337952
- 0.012140560656553135
- 0.012302828935207798
- 0.012356829503551126
- 0.012258464761544019
- 0.01225408855243586
- 0.012220123712904752
- 0.012113819154910744
- 0.012252881051972508
- 0.012080564582720398
- 0.012105555506423116
- 0.011999907885910942
- 0.012144036893732845
- 0.011806387372780591
- 0.012024109461344778
- 0.012370256375288591
- 0.012043748411815613
- 0.011982276232447475
- 0.01213816138333641
- 0.012023818760644645
- 0.012014380691107362
- 0.011912805412430316
- 0.012002172728534788
- 0.011908868921454995
- 0.011972820677328855
- 0.011997326149139554
- 0.011935924686258658
- 0.01199068034766242
- 0.011968726443592459
- 0.012066271039657295
- 0.012395145406480878
- 0.012078751577064395
- 0.01174912711721845
- 0.011985146318329499
- 0.01184524543932639
- 0.011963207751978188
- 0.012053990137064829
- 0.011898223555181175
- 0.012207657832186669
- 0.011974317382555454
- 0.012031778128584848
- 0.012285390228498727
- 0.011787675716914237
- 0.01223472427809611
- 0.012048324599163607
- 0.011864156456431374
- 0.01201965012587607
- 0.01192362374276854
- 0.011690042313421144
- 0.011944618553388863
- 0.01186227852012962
- 0.011939380853436887
- 0.011953723133774474
- 0.01194032810162753
- 0.011932268820237369
- 0.011938836588524283
- 0.012025326245930047
- 0.011954713764134794
