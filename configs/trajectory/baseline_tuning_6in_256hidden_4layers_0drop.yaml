name: baseline_tuning_6in_256hidden_4layers_0drop
frames_in: 6
frames_out: 15
layers: 4
hidden_size: 256
dropout: 0.0
loss: MSE loss
optimizer: Adam
learning rate: 0.002
weight decay: 0.0001
epochs: 100
batch_size: 32
train loss:
- 0.03700403133407235
- 0.021727292728610337
- 0.018626151699572803
- 0.017539074236992747
- 0.016796531481668354
- 0.015597425890155137
- 0.014015592215582729
- 0.013336267927661538
- 0.013091390021145344
- 0.012620948476251215
- 0.011981893528718502
- 0.012115952261956409
- 0.01208943712990731
- 0.011878029309445991
- 0.011754348035901785
- 0.01171650547767058
- 0.011786340305116027
- 0.011571120168082417
- 0.0117771735007409
- 0.011481085943523794
- 0.011448781442595646
- 0.011584741267142817
- 0.011615709209581836
- 0.011270689184311778
- 0.011414827045518905
- 0.011586136103142053
- 0.011267288291128353
- 0.011336994881276041
- 0.01136668940889649
- 0.011518967617303134
- 0.011342667287681251
- 0.011259527219226583
- 0.011286726198159158
- 0.011124789429595694
- 0.011167715775081887
- 0.011306125111877919
- 0.011142210441175849
- 0.011253537551965564
- 0.011108923488063738
- 0.011383254575775936
- 0.011446506815264
- 0.01129246111959219
- 0.011000284645706414
- 0.01106701375101693
- 0.011161012679804117
- 0.011136888584587723
- 0.011229469429235906
- 0.01105851725442335
- 0.011157952051144093
- 0.010958483297144994
- 0.011171730112982914
- 0.011068999499548227
- 0.011079632549080998
- 0.011012035794556141
- 0.011018507723929361
- 0.011102779558859766
- 0.011105175240663812
- 0.011205241794232278
- 0.011099226132500917
- 0.011169560527196153
- 0.011036255880026147
- 0.01123826538096182
- 0.011013781919609755
- 0.011095841036876664
- 0.011054308584425599
- 0.011094160506036132
- 0.011041487252805382
- 0.010998455691151321
- 0.011036228737793863
- 0.010959494224516675
- 0.011010080424603075
- 0.010994684614706785
- 0.010915834357729182
- 0.010991667018970475
- 0.011105703888460994
- 0.010996782744769007
- 0.010907382867299021
- 0.010964339756174012
- 0.010857270157430321
- 0.01104925122926943
- 0.011181858746567741
- 0.010907817992847412
- 0.011008838866837322
- 0.010847223148448392
- 0.010947997763287276
- 0.010873325949069113
- 0.011007799999788404
- 0.011151770484866574
- 0.010935604973929003
- 0.011103301029652356
- 0.0109098244109191
- 0.010964457743102684
- 0.01092322114855051
- 0.010884519602404908
- 0.010937208664836362
- 0.010889609553851187
- 0.011016055481741205
- 0.010864660405786708
- 0.010850691457744688
- 0.01097096733865328
