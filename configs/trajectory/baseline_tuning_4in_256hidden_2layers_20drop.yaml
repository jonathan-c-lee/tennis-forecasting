name: baseline_tuning_4in_256hidden_2layers_20drop
frames_in: 4
frames_out: 15
layers: 2
hidden_size: 256
dropout: 0.2
loss: MSE loss
optimizer: Adam
learning rate: 0.002
weight decay: 0.0001
epochs: 100
batch_size: 32
train loss:
- 0.03417242748409877
- 0.01732830005341474
- 0.014365343043557655
- 0.013366346718904413
- 0.012720907503670012
- 0.011972272389188961
- 0.011705256308670397
- 0.011593446193009983
- 0.01112438999346377
- 0.010848298617121246
- 0.010199751181780924
- 0.009907591875274608
- 0.009368161164960007
- 0.009368573844340849
- 0.008694942316247357
- 0.009009216380317086
- 0.00861865879944813
- 0.008603488267571837
- 0.008522379091354433
- 0.008195068342266259
- 0.008426521933511084
- 0.007890138703248934
- 0.007993547527556434
- 0.007976961966006108
- 0.007936290764238364
- 0.007862439956860963
- 0.007829124452891174
- 0.007896506152817128
- 0.007904921244415972
- 0.007737969594467201
- 0.007622374342409549
- 0.007822963081438232
- 0.007631270853043707
- 0.00792230529434703
- 0.007833666898264193
- 0.007776228251096643
- 0.007605571483756289
- 0.007671567398686836
- 0.007585876190138084
- 0.0077477284868098335
- 0.007644650003194441
- 0.007767708747889157
- 0.007620587490820958
- 0.007451183967476274
- 0.007621928141164927
- 0.007720286273799929
- 0.007375274270336017
- 0.0073646113598420295
- 0.0074606578521154545
- 0.007300538125873348
- 0.007755006134602023
- 0.007483452184838645
- 0.0074610114097595215
- 0.007491543153185903
- 0.007404777209883855
- 0.007363570112459085
- 0.007584079967834699
- 0.007391048693040639
- 0.007478927842766782
- 0.0073770415095359455
- 0.007476604704595643
- 0.007446390450184728
- 0.007519537111583315
- 0.007666271507602047
- 0.0074932218046376
- 0.007288908080002408
- 0.007557129027482904
- 0.007614316200308594
- 0.007318614878588253
- 0.007334927085465119
- 0.0074330253676039935
- 0.007236606950216272
- 0.00760864116895346
- 0.007315048740969764
- 0.007397067244452091
- 0.007317883014265034
- 0.007365078625073771
- 0.0071965348101968385
- 0.0072368519541658
- 0.007389234223713477
- 0.007369103203355162
- 0.007234334750215949
- 0.007228689388958392
- 0.0074777649313119456
- 0.007376127703874199
- 0.007332726059780445
- 0.007354185491064449
- 0.007267131447148176
- 0.007421650693832356
- 0.007136251806155031
- 0.0074341758959179305
- 0.007370244142495924
- 0.007288255314860079
- 0.007316830999183434
- 0.007336414522594876
- 0.007329527751462143
- 0.007379370442603106
- 0.007121591817265675
- 0.007258621820559104
- 0.00717024010732586
