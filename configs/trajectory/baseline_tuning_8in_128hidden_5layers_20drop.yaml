name: baseline_tuning_8in_128hidden_5layers_20drop
frames_in: 8
frames_out: 15
layers: 5
hidden_size: 128
dropout: 0.2
loss: MSE loss
optimizer: Adam
learning rate: 0.002
weight decay: 0.0001
epochs: 100
batch_size: 32
train loss:
- 0.03628970439769799
- 0.023185177788704256
- 0.02341922940804234
- 0.02328712822091353
- 0.023275966284490083
- 0.023257645684046837
- 0.023298373658068573
- 0.023238835644118393
- 0.023267487700604186
- 0.023108978578938715
- 0.023391658711376825
- 0.023178470262055155
- 0.02321419169348252
- 0.023042128338843962
- 0.023303534328654597
- 0.02320738663731874
- 0.02310060268810278
- 0.023088369577462915
- 0.023091189103507544
- 0.023091544397175312
- 0.02318391475024857
- 0.023105334101504164
- 0.02303638199603633
- 0.023069277040283137
- 0.023139430634394477
- 0.02307701492799988
- 0.02308194096424157
- 0.023180002547046052
- 0.02308616274378345
- 0.023025617168485363
- 0.023088648820979687
- 0.02301512215333649
- 0.023036706155236765
- 0.023263253672402118
- 0.02313445958815798
- 0.023033983946506735
- 0.023085595509391044
- 0.022999107366121267
- 0.023119527068507822
- 0.02305663176635398
- 0.023041554891704757
- 0.023133098749043065
- 0.02313480087662045
- 0.023046768176121803
- 0.02303100996238144
- 0.023117988782970212
- 0.02308425606711756
- 0.023098711018698124
- 0.023052842863187005
- 0.023019845727123792
- 0.023120791474475138
- 0.023071183316126655
- 0.02299342610979382
- 0.023039614145137086
- 0.023104184483991392
- 0.023089427111929732
- 0.02302166574363467
- 0.023136713907499856
- 0.023029594623212572
- 0.023063177796953088
- 0.023071567256805262
- 0.02318210258514066
- 0.023057477501562878
- 0.02308815208416951
- 0.023061908085983764
- 0.02301435700700253
- 0.023050240240991116
- 0.02305966485999053
- 0.023093832208763196
- 0.023047589225388024
- 0.023051398311140416
- 0.02304507428898087
- 0.02298248170203046
- 0.023095210424706906
- 0.023055869319676597
- 0.023046845923873443
- 0.023054719584274897
- 0.023003717907999134
- 0.023036425897875166
- 0.023106907942344115
- 0.02307797560491894
- 0.023027173891852173
- 0.023013491961586325
- 0.022995414471701732
- 0.02300119596898933
- 0.023030304809725736
- 0.022955157039569147
- 0.0231346488706296
- 0.02303684847075728
- 0.023106368117128746
- 0.0229479465162075
- 0.023057968497181995
- 0.023020896577288077
- 0.023027579311894465
- 0.022991451753091207
- 0.02306121060789763
- 0.02299901755833173
- 0.022995276129038275
- 0.02305249448959963
- 0.022971086793496638
