name: baseline_tuning_4in_256hidden_3layers_0drop
frames_in: 4
frames_out: 15
layers: 3
hidden_size: 256
dropout: 0.0
loss: MSE loss
optimizer: Adam
learning rate: 0.002
weight decay: 0.0001
epochs: 100
batch_size: 32
train loss:
- 0.03552646072650397
- 0.019540616633071575
- 0.016705726342344726
- 0.014567183951536814
- 0.013343916119386753
- 0.013372055829767093
- 0.012561857286426757
- 0.012778933142760285
- 0.012302748082826534
- 0.012232711009773207
- 0.011791432954738905
- 0.011396038817402757
- 0.011353620121048557
- 0.010739643212959723
- 0.01031191883907642
- 0.010157663070447282
- 0.009681020593155681
- 0.008976161054530998
- 0.008594866405114716
- 0.008514733398678127
- 0.008449617600827306
- 0.008329886040523832
- 0.00798814992684825
- 0.007952843367317577
- 0.008094346622166074
- 0.007948496884861846
- 0.007874351811169841
- 0.007934244665788648
- 0.007845377428802076
- 0.00788243964061509
- 0.0078178350986154
- 0.007697418067650294
- 0.007604219076902043
- 0.007496433763730305
- 0.0076602216275339875
- 0.007413217337963022
- 0.007673122281967489
- 0.007557339655856292
- 0.007542735126651364
- 0.007562566376118748
- 0.007448294855378292
- 0.007387265315808264
- 0.007439985192944238
- 0.007561771791244363
- 0.007236731635337627
- 0.007206353436534603
- 0.007224632634056939
- 0.007389768348707829
- 0.007227697415438331
- 0.007284719341744979
- 0.0072661386630325404
- 0.007187547599091942
- 0.007121141149122038
- 0.007251814450424762
- 0.007359172550984371
- 0.007353033634753507
- 0.007396034764148939
- 0.007190172563387472
- 0.0070164317702069694
- 0.007107696030288935
- 0.007271978739695048
- 0.007186788536700202
- 0.0071436450701712825
- 0.007162531501510077
- 0.007192037556964306
- 0.007159271771893089
- 0.007153743708414244
- 0.007089235716395908
- 0.007189416993455386
- 0.0070618451572954655
- 0.007120969049908497
- 0.007001308847885625
- 0.007180900778621435
- 0.0070512950259410305
- 0.0070578915248107575
- 0.007120023843728834
- 0.007277084968662188
- 0.0070535761856094555
- 0.00705652679949079
- 0.007083765288561951
- 0.00697766671842539
- 0.007013175142124111
- 0.0071827674939952514
- 0.007047817208746701
- 0.00695830727007562
- 0.006944634894162048
- 0.007063535869949394
- 0.007304145126707024
- 0.007012584263941756
- 0.006981824133775116
- 0.006996204950099374
- 0.007045477766681601
- 0.007107707556843022
- 0.006891174448861016
- 0.00695237070666971
- 0.006897589605715539
- 0.007092279330309894
- 0.00696117174524216
- 0.007129867998078281
- 0.007229122534440255
