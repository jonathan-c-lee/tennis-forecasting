name: baseline_tuning_8in_128hidden_2layers_40drop
frames_in: 8
frames_out: 15
layers: 2
hidden_size: 128
dropout: 0.4
loss: MSE loss
optimizer: Adam
learning rate: 0.002
weight decay: 0.0001
epochs: 100
batch_size: 32
train loss:
- 0.03449778480431701
- 0.019197409842870657
- 0.015203709395814545
- 0.012990034079249901
- 0.01185315891015756
- 0.010865965021090418
- 0.009882355857451883
- 0.009521934121281286
- 0.009354552259975219
- 0.009105446377085357
- 0.009035705635913565
- 0.009042864771489101
- 0.008649778322469963
- 0.00836718914723849
- 0.008852450430534686
- 0.00842046807814814
- 0.008604320603175253
- 0.00814869549549833
- 0.008019586624223975
- 0.007963825111524969
- 0.008131060425097807
- 0.00777256598436757
- 0.007418767920447679
- 0.007335941808274652
- 0.007198877697456864
- 0.007002656315630184
- 0.007088217421588075
- 0.006958348788556796
- 0.00707937333779999
- 0.006978417534522618
- 0.0067500362491022935
- 0.006827296220544207
- 0.006849101225880882
- 0.006684516326677573
- 0.006513987838797555
- 0.006489673449622491
- 0.006568975522642648
- 0.006511318385365266
- 0.006625234007929699
- 0.0064365351160021525
- 0.0064528301753150885
- 0.00652656298649462
- 0.0064118721911424325
- 0.006451668170622632
- 0.006648334774742776
- 0.006399463151899885
- 0.006489822758903987
- 0.006394431195398675
- 0.006377723981638121
- 0.006209518070792472
- 0.006328846569632804
- 0.00632765649263806
- 0.00641048603537905
- 0.006291420532013231
- 0.00621784228796163
- 0.006334494881897788
- 0.006357350749187643
- 0.006379800892871203
- 0.006245147537062817
- 0.006222098379077602
- 0.006235441979893212
- 0.0060924308056224
- 0.006189803398343958
- 0.006096351199158573
- 0.006213764833946583
- 0.006162979460286
- 0.0062324071444476705
- 0.006184682895464799
- 0.006130810090210996
- 0.006120591433247244
- 0.0060854042420470265
- 0.006141514906400367
- 0.006138177240809685
- 0.00601243235972486
- 0.006006143789714839
- 0.006225028405366819
- 0.006085449198991815
- 0.006022453266844342
- 0.0061060368419400875
- 0.0062534322378614655
- 0.00605864493070241
- 0.005952475224091093
- 0.006058270497766288
- 0.006019243705263243
- 0.005949294412673651
- 0.006046014861498452
- 0.006033477559096262
- 0.005984734885300262
- 0.006053827493746258
- 0.005915615984674873
- 0.006021373115385635
- 0.005845917450688496
- 0.005960229784250259
- 0.006100623467583445
- 0.005924028208880107
- 0.006070342289189561
- 0.006072415513446248
- 0.00614668923736656
- 0.005960418371177173
- 0.005948597494559952
