name: baseline_tuning_4in_64hidden_5layers_20drop
frames_in: 4
frames_out: 15
layers: 5
hidden_size: 64
dropout: 0.2
loss: MSE loss
optimizer: Adam
learning rate: 0.002
weight decay: 0.0001
epochs: 100
batch_size: 32
train loss:
- 0.048608056740996275
- 0.023703918228914708
- 0.02361044825779067
- 0.023356290982553252
- 0.023566134664931414
- 0.023327531718453507
- 0.02357537947871067
- 0.023309009529098316
- 0.023443042956016683
- 0.02358986277501156
- 0.023439018308748432
- 0.023416153971611717
- 0.02338994089743973
- 0.02335110186012807
- 0.023334003651491655
- 0.023295332390217135
- 0.023297508822087153
- 0.023379459862171867
- 0.02329600679423706
- 0.023264759772077755
- 0.02336624760272694
- 0.023546151195963223
- 0.023256438611834136
- 0.023284805984592732
- 0.02332657677937805
- 0.023308933172145007
- 0.0233335851887126
- 0.023362398170578627
- 0.023200898339259035
- 0.023499768411303745
- 0.023363276453757728
- 0.02340025263895959
- 0.023309956569178603
- 0.023153076408269965
- 0.023287854812763357
- 0.023127847305142585
- 0.023195453724007546
- 0.023158309026908727
- 0.02340385986974946
- 0.023185814926285804
- 0.02325124300464436
- 0.023357426356754186
- 0.023264191055923332
- 0.023253565044774684
- 0.02333919989106096
- 0.02334656295033149
- 0.02353672759124526
- 0.023334785238092327
- 0.02320205104065898
- 0.023349926450554236
- 0.023276711526660273
- 0.023253156585089953
- 0.023273958870566186
- 0.023165199687175543
- 0.023065486439952144
- 0.0231935494567877
- 0.02313384205608824
- 0.023175618208852816
- 0.023143067473062762
- 0.023269891451446364
- 0.023158076966618313
- 0.023272544283557822
- 0.023170274245426243
- 0.02324633713075776
- 0.023333159976719337
- 0.023262497635535253
- 0.023210962337476236
- 0.02323486669747918
- 0.023152478844111347
- 0.023497678385472592
- 0.023430984260307416
- 0.023193750668455054
- 0.023269664507681205
- 0.023279771024798168
- 0.023066857185812643
- 0.023279583771471626
- 0.023328383016273564
- 0.023245439485267357
- 0.023217468281035072
- 0.023090461728933417
- 0.023516959119818093
- 0.023187774095546315
- 0.02317381933055542
- 0.02322155894872583
- 0.02326873448435907
- 0.023173206278847322
- 0.023422816112913466
- 0.023150010632328046
- 0.02326545813753281
- 0.023253727221378574
- 0.02332487043545202
- 0.023239439813259207
- 0.023207359314884667
- 0.02327058111305958
- 0.023211454271258397
- 0.02320146408897859
- 0.02323542618089252
- 0.02329854671785861
- 0.02318415179112811
- 0.023250779608425535
