name: baseline_tuning_4in_128hidden_5layers_20drop
frames_in: 4
frames_out: 15
layers: 5
hidden_size: 128
dropout: 0.2
loss: MSE loss
optimizer: Adam
learning rate: 0.002
weight decay: 0.0001
epochs: 100
batch_size: 32
train loss:
- 0.04000593919628932
- 0.02336124867706755
- 0.023527473970143882
- 0.023613049729186812
- 0.023474737771867232
- 0.023551231250166893
- 0.023324974571113235
- 0.0236363073029084
- 0.023390168830016513
- 0.023282826636676437
- 0.02349789840755639
- 0.023568256056787057
- 0.023522587389581733
- 0.02329397800573964
- 0.023288548429623062
- 0.023322443753756857
- 0.023386168891541017
- 0.023346510083403115
- 0.023418903488803794
- 0.02316097119524155
- 0.023217776111890503
- 0.023442191842161578
- 0.023317312764256824
- 0.02334416250663775
- 0.023252018140973867
- 0.02326205947700842
- 0.023514511477616098
- 0.023335789640744526
- 0.023338189877477693
- 0.023133083200657073
- 0.023355091002160384
- 0.023321840061265746
- 0.02337026170650382
- 0.023239393856514384
- 0.023291839746597372
- 0.023292053421890294
- 0.023283702746769528
- 0.02321856792004388
- 0.023403037017510262
- 0.02324716677820241
- 0.02321359446203267
- 0.023201969900616893
- 0.023213590092865038
- 0.023352143596167916
- 0.023147080277587162
- 0.023145749578597368
- 0.0232755811540065
- 0.02321799915789822
- 0.02322403461108973
- 0.023224074784436343
- 0.023389780732952517
- 0.023186314345141987
- 0.023156017306502217
- 0.023126783125378466
- 0.023186342583762273
- 0.023356837438948362
- 0.02329290542107673
- 0.02313878835627326
- 0.023320307000827642
- 0.023226272993157677
- 0.023215813056370358
- 0.023359809438755482
- 0.023278328291151996
- 0.02336639901738108
- 0.02326741055757911
- 0.023265784744311263
- 0.023503705951166742
- 0.02318926417717227
- 0.023112026158214352
- 0.02328297756842257
- 0.023124250686830945
- 0.023272831314875755
- 0.02341510323278698
- 0.02330763341376075
- 0.02344438875163043
- 0.023315590369388645
- 0.023423398925382415
- 0.023273641726485005
- 0.023444921905059875
- 0.02325652355765119
- 0.023221244092708752
- 0.023216247386126605
- 0.023299785491860944
- 0.02333144643516452
- 0.023191643384411747
- 0.023297052290060637
- 0.02320465805399933
- 0.02316144819336909
- 0.02319518989526931
- 0.023243031533503974
- 0.02316029798699382
- 0.023578255793746606
- 0.02311015906341282
- 0.023172293030829343
- 0.023295550446175498
- 0.023237460856268436
- 0.023401545452666872
- 0.023185792057142583
- 0.02320474015985742
- 0.023270697827324455
