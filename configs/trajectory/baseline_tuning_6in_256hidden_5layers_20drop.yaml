name: baseline_tuning_6in_256hidden_5layers_20drop
frames_in: 6
frames_out: 15
layers: 5
hidden_size: 256
dropout: 0.2
loss: MSE loss
optimizer: Adam
learning rate: 0.002
weight decay: 0.0001
epochs: 100
batch_size: 32
train loss:
- 0.036084724008105695
- 0.02339560155523941
- 0.023444081842899322
- 0.02373135203961283
- 0.02396774578373879
- 0.023301236191764475
- 0.023261010530404747
- 0.02352609015069902
- 0.023293999501038342
- 0.023301657545380293
- 0.023155569843947888
- 0.023268038569949568
- 0.023346130223944783
- 0.023367855534888803
- 0.023302689229603858
- 0.023219083657022566
- 0.023175407212693244
- 0.02344933336135
- 0.023192714469041677
- 0.02333495272323489
- 0.023256828356534243
- 0.02316273795440793
- 0.02323979320935905
- 0.023299030493944883
- 0.02318594844546169
- 0.023429781512822954
- 0.02317569504957646
- 0.023137286817654968
- 0.023239859973546117
- 0.023138779238797723
- 0.023153415042907
- 0.02322807281743735
- 0.023210143751930444
- 0.02316272599855438
- 0.023160536028444766
- 0.023172348097432405
- 0.02320853224955499
- 0.023150884709320963
- 0.02323325015604496
- 0.023178363125771283
- 0.023167366720736028
- 0.023095245705917478
- 0.02315040979301557
- 0.02321740654297173
- 0.023252343130297958
- 0.02318931740010157
- 0.023275900341104716
- 0.023210109188221396
- 0.023120720486622302
- 0.02313885889016092
- 0.02317257283721119
- 0.023222442250698805
- 0.023166480660438537
- 0.023134432756341992
- 0.023126281320583076
- 0.023151067248545588
- 0.02320750947110355
- 0.023151238437276334
- 0.023220348893664776
- 0.023084187612403183
- 0.023173837643116712
- 0.02312313565053046
- 0.023196707980241628
- 0.023202159570064395
- 0.023142609174828976
- 0.023174926242791116
- 0.023261437402106823
- 0.023126571858301758
- 0.023078413237817584
- 0.023254255845677106
- 0.02320821098983288
- 0.023212296585552395
- 0.023229927034117283
- 0.02320188641315326
- 0.02307673601899296
- 0.023211118509061636
- 0.02312239761231467
- 0.02315515170339495
- 0.023161987180355937
- 0.023183554271236063
- 0.02321925078285858
- 0.023171469941735266
- 0.023198722442612053
- 0.023219640098977833
- 0.023137172334827483
- 0.023111422662623227
- 0.02317812987603247
- 0.023106499528512358
- 0.023154329298995434
- 0.023172104940749706
- 0.023093187413178385
- 0.02317784079350531
- 0.023156443936750294
- 0.02316301184473559
- 0.0231494199950248
- 0.02319441419094801
- 0.023141388385556638
- 0.02314020737539977
- 0.023165562469512225
- 0.02309115424286574
