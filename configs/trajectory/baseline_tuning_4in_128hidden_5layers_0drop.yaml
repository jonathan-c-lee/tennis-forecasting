name: baseline_tuning_4in_128hidden_5layers_0drop
frames_in: 4
frames_out: 15
layers: 5
hidden_size: 128
dropout: 0.0
loss: MSE loss
optimizer: Adam
learning rate: 0.002
weight decay: 0.0001
epochs: 100
batch_size: 32
train loss:
- 0.0398468292651721
- 0.023712183877733755
- 0.023552933063955953
- 0.02343844141765141
- 0.02337817797138367
- 0.023467540878940513
- 0.024037742941283885
- 0.0235591433758353
- 0.02335634866706383
- 0.02342181719471643
- 0.02365051512318997
- 0.023718708298272557
- 0.023655356218417484
- 0.02361216179934549
- 0.02323834532526908
- 0.023234064046891382
- 0.02324688508186811
- 0.023560522687563926
- 0.023216408010526202
- 0.02334260119608155
- 0.023261782391295758
- 0.02329080962334518
- 0.02334219378269749
- 0.023476764950671313
- 0.023348564626993955
- 0.023529913897315662
- 0.023270017179993937
- 0.023200003936150928
- 0.023197663384547203
- 0.02329811998815448
- 0.023365638793710574
- 0.023275445778796703
- 0.02346323888150998
- 0.023326752799344652
- 0.023211249000864263
- 0.023429649962503234
- 0.02346606228362631
- 0.023330874189182563
- 0.023303896924595772
- 0.023160864794511855
- 0.023272007979728556
- 0.023349944341145915
- 0.02335346092320519
- 0.02323173848843133
- 0.023153176692164976
- 0.023131072475218478
- 0.023265300709524272
- 0.023451092411522514
- 0.023491180386900165
- 0.02323296838612468
- 0.023395450923729827
- 0.02324391387540985
- 0.023078832200463906
- 0.023217549524557443
- 0.023105787400754144
- 0.023160005413731675
- 0.023227521551795947
- 0.023092736938485393
- 0.02328546798247614
- 0.023445857274863455
- 0.023369972168663402
- 0.023201525855211565
- 0.023218316129512258
- 0.023159484321504463
- 0.02320717318834346
- 0.02315618287495993
- 0.02328616176030518
- 0.023227744241371567
- 0.023296357822363025
- 0.023272818483320284
- 0.023295300587275882
- 0.023233598914503314
- 0.023286029868931683
- 0.023202915020562983
- 0.023348107221133917
- 0.023397503995601043
- 0.023166851060074052
- 0.02328491266126986
- 0.02325888103403059
- 0.02313497203781649
- 0.023322092714133085
- 0.023281273696525596
- 0.023175289233525593
- 0.023324397381073163
- 0.023230420241569294
- 0.023299494965208903
- 0.023268629267903757
- 0.023205882352259424
- 0.02323479029453463
- 0.023159717911003547
- 0.023212868214389424
- 0.02315861210916881
- 0.023113719866047672
- 0.023412620257816198
- 0.02345262855337358
- 0.02320709708434196
- 0.02325259298546079
- 0.023236042176039865
- 0.02333331095445671
- 0.023491966250686
